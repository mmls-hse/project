{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tokenizers import Tokenizer, models, trainers, processors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_tokenizer(texts):\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "    trainer = trainers.BpeTrainer(vocab_size=50000, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "    tokenizer.train_from_iterator(texts, trainer)\n",
        "    tokenizer.post_processor = processors.TemplateProcessing(\n",
        "        single=\"<sos> $A <eos>\",\n",
        "        pair=\"<sos> $A <eos> $B:1 <eos>\",\n",
        "        special_tokens=[(\"<sos>\", 1), (\"<eos>\", 2)]\n",
        "    )\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        self.texts = dataframe['text'].tolist()\n",
        "        self.summaries = dataframe['summary'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = torch.tensor(self.tokenizer.encode(self.texts[idx]).ids)\n",
        "        summary = torch.tensor(self.tokenizer.encode(self.summaries[idx]).ids)\n",
        "        return text, summary\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, summaries = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value=0)\n",
        "    return texts_padded, summaries_padded\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        embedded_src = self.embedding(src)\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded_src)\n",
        "\n",
        "        embedded_trg = self.embedding(trg)\n",
        "        decoder_outputs, _ = self.decoder(embedded_trg, (hidden, cell))\n",
        "\n",
        "        output = self.fc(decoder_outputs)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, tokenizer, num_epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for src, trg in tqdm(dataloader):\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(src, trg[:, :-1])\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}')\n",
        "\n",
        "# Функция для генерации резюме\n",
        "def generate_summary(model, tokenizer, text, max_len=100):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([tokenizer.encode(text).ids]).to(device)\n",
        "\n",
        "        embedded = model.embedding(input_tensor)\n",
        "        encoder_outputs, (hidden, cell) = model.encoder(embedded)\n",
        "\n",
        "        trg_indexes = [tokenizer.token_to_id('<sos>')]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            trg_tensor = torch.tensor([trg_indexes]).to(device)\n",
        "            embedded_trg = model.embedding(trg_tensor)\n",
        "\n",
        "            output, (hidden, cell) = model.decoder(embedded_trg, (hidden, cell))\n",
        "            prediction = model.fc(output[:, -1, :]).argmax(1).item()\n",
        "\n",
        "            if prediction == tokenizer.token_to_id('<eos>'):\n",
        "                break\n",
        "\n",
        "            trg_indexes.append(prediction)\n",
        "\n",
        "        return tokenizer.decode(trg_indexes[1:])"
      ],
      "metadata": {
        "id": "ptqQfXQL-zes"
      },
      "id": "ptqQfXQL-zes",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/train_data20000.csv\")\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "summaries = df['summary'].tolist()\n",
        "\n",
        "tokenizer = train_tokenizer(texts)"
      ],
      "metadata": {
        "id": "RdVI7kgl-24B"
      },
      "id": "RdVI7kgl-24B",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(df, tokenizer)\n",
        "\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "embedding_dim = 128\n",
        "hidden_size = 128\n",
        "model = Seq2Seq(vocab_size, embedding_dim, hidden_size)"
      ],
      "metadata": {
        "id": "6C11NEy0HS6Q"
      },
      "id": "6C11NEy0HS6Q",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJqHN5fXRzHC",
        "outputId": "bc47108c-b573-48cd-e34a-410195ad00ad"
      },
      "id": "lJqHN5fXRzHC",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "M2ki2OnrR9yY"
      },
      "id": "M2ki2OnrR9yY",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, tokenizer, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvW6ypWO-62_",
        "outputId": "8c02b8fe-49b2-4605-cd35-28dfe624369f"
      },
      "id": "bvW6ypWO-62_",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:07<00:00,  4.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 9.5212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:07<00:00,  4.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 9.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:05<00:00,  4.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 8.4633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:08<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 7.9181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:06<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 7.4330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:08<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 7.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:08<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 6.6447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:08<00:00,  4.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 6.3237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:07<00:00,  4.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 6.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [02:07<00:00,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 5.7895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_summary = \"По информации ведомства, лодки прибыли ранним утром в среду. Это первый случай в ходе нынешнего кризиса с мигрантами в Европе, когда группы беженцев прибывают на территорию, подконтрольную Британии. База Акротири, расположенная на южном берегу острова, используется для боевых вылетов британской авиации, наносящей удары по позициям боевиков экстремистского движения \"\"Исламское государство\"\" в Ираке. Ранее мигранты высаживались в основном на островах Греции и Италии, обходя Кипр стороной. С начала года в европейские страны морским путем прибыли около 600 тысяч мигрантов. Основная часть беженцев прибывает из Афганистана, Сирии и Ирака. Многие идут пешком из Турции через Грецию, Македонию и Сербию.\"\n",
        "generated_summary = generate_summary(model, tokenizer, reference_summary)\n",
        "print(f\"Generated summary: {generated_summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLOC4sLc_Ciu",
        "outputId": "94a7cc85-1592-4882-aeb2-21e7f216b72a"
      },
      "id": "xLOC4sLc_Ciu",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summary: В  Москве  задержан  главный  санитар ный  врач  РФ  Анна  Попова .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference_tokens = [nltk.word_tokenize(reference.lower())]\n",
        "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "    return sentence_bleu(reference_tokens, candidate_tokens)\n",
        "\n",
        "\n",
        "def calculate_meteor(reference, candidate):\n",
        "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "    return meteor_score([reference_tokens], candidate_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C90a4zfkB9kS",
        "outputId": "6f6fd6b0-3f58-413f-f10b-80e137a3edef"
      },
      "id": "C90a4zfkB9kS",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8DJv8pUDJCn",
        "outputId": "3e37ea40-8be2-46b4-d97d-5b7ca12e6881"
      },
      "id": "o8DJv8pUDJCn",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge(reference, candidate):\n",
        "    # Вычисление ROUGE\n",
        "    scores = scorer.score(reference, candidate)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "nsExKjgoDH5f"
      },
      "id": "nsExKjgoDH5f",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = calculate_bleu(reference_summary, generated_summary)\n",
        "print(f\"BLEU score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vchJsKaqCJjL",
        "outputId": "249f60d4-0ae7-40fb-dfc9-f894523db1e8"
      },
      "id": "vchJsKaqCJjL",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meteor_score_value = calculate_meteor(reference_summary, generated_summary)\n",
        "print(f\"METEOR score: {meteor_score_value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrIVi9WzCzFA",
        "outputId": "251361bd-af0f-4c9f-e8d3-18118fdcbcff"
      },
      "id": "mrIVi9WzCzFA",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.0097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
        "for key, value in rouge_scores.items():\n",
        "    print(f\"{key}: precision={value.precision:.4f}, recall={value.recall:.4f}, fmeasure={value.fmeasure:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUE2tB8oC1Uv",
        "outputId": "9f922552-64ef-4a05-ad18-76effe39d0c3"
      },
      "id": "tUE2tB8oC1Uv",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge1: precision=0.0000, recall=0.0000, fmeasure=0.0000\n",
            "rouge2: precision=0.0000, recall=0.0000, fmeasure=0.0000\n",
            "rougeL: precision=0.0000, recall=0.0000, fmeasure=0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_perplexity(model, tokenizer, text, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "    input_ids = torch.tensor([tokens]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids[:, :-1], input_ids[:, 1:])\n",
        "        logits = outputs\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    nll = 0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        nll += -log_probs[0, i, tokens[i + 1]].item()\n",
        "\n",
        "    perplexity = torch.exp(torch.tensor(nll / (len(tokens) - 1)))\n",
        "    return perplexity.item()"
      ],
      "metadata": {
        "id": "SHCi3jH6DlXw"
      },
      "id": "SHCi3jH6DlXw",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = calculate_perplexity(model, tokenizer, reference_summary)\n",
        "print(f\"Perplexity: {perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iUEsXCaEH4m",
        "outputId": "9a92e325-a05f-497b-f971-b509e6aaa570"
      },
      "id": "8iUEsXCaEH4m",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 337824.8438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rAU0DCJdZ0k"
      },
      "id": "7rAU0DCJdZ0k",
      "execution_count": 112,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}